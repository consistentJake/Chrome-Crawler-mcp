"""
Demo showing how Claude Code can integrate with the Web Extraction Tool
This demonstrates the complete workflow for intelligent web automation
"""

import json
import re
from web_extraction_tool import WebExtractionTool


def claude_code_workflow_simulation():
    """
    Simulate how Claude Code would use the Web Extraction Tool
    to identify patterns and extract forum posts automatically
    """
    
    print("ðŸš€ Claude Code Web Extraction Workflow Simulation")
    print("=" * 60)
    
    # Step 1: Initialize tool with conservative token limits for Claude Code
    tool = WebExtractionTool(max_tokens=4000)
    
    # Step 2: Load test HTML directly (simulating what Claude Code would do)
    print("\nðŸ“– Step 1: Loading and analyzing test page...")
    
    with open('test/page.html', 'r', encoding='gbk') as f:
        html_content = f.read()
    
    # Sanitize content for Claude Code analysis
    sanitized_result = tool.sanitizer.sanitize(html_content, extraction_mode='links')
    
    print(f"   âœ… Sanitized HTML: {len(sanitized_result['sanitized_html'])} characters")
    print(f"   âœ… Found {len(sanitized_result['element_registry'])} interactive elements")
    print(f"   âœ… Estimated tokens: {sanitized_result['statistics']['estimated_tokens']}")
    
    # Step 3: Show what Claude Code would see
    print("\nðŸ¤– Step 2: Claude Code Analysis Input")
    print("-" * 40)
    
    indexed_text = sanitized_result['indexed_text']
    lines = indexed_text.split('\n')
    
    # Show sample of indexed text that Claude Code would analyze
    print("Sample indexed elements (first 10):")
    for line in lines[:10]:
        print(f"   {line}")
    
    print(f"\n   ... and {len(lines)-10} more elements")
    
    # Step 4: Simulate Claude Code's pattern recognition
    print("\nðŸ§  Step 3: Claude Code Pattern Analysis")
    print("-" * 40)
    
    # This simulates what Claude Code would identify
    thread_links = []
    for i, line in enumerate(lines):
        if 'thread-' in line and 'href=' in line:
            # Extract href and text
            href_match = re.search(r'href="([^"]*)"', line)
            text_match = re.search(r'>([^<]*)</', line)
            
            if href_match:
                href = href_match.group(1)
                text = text_match.group(1) if text_match else ''
                thread_links.append({
                    'index': i,
                    'href': href,
                    'title': text.strip(),
                    'line': line
                })
    
    print(f"   ðŸ” Claude Code identified pattern: 'thread-{'{id}'}-1-1.html'")
    print(f"   ðŸ“Š Found {len(thread_links)} matching thread links")
    
    # Step 5: Show Claude Code's recommended selector
    print("\nðŸŽ¯ Step 4: Claude Code Selector Recommendation")
    print("-" * 40)
    
    # Claude Code would analyze the patterns and recommend these selectors
    recommended_selectors = [
        "a[href*='thread-'][href*='-1-1.html']",  # Most specific
        "a[href^='thread-']",                      # Broader match
        "a[href*='thread-'][href$='.html']"       # Pattern-based
    ]
    
    for i, selector in enumerate(recommended_selectors, 1):
        print(f"   Option {i}: {selector}")
    
    # Step 6: Show extracted posts with titles
    print("\nðŸ“ Step 5: Extracted Forum Posts")
    print("-" * 40)
    
    print("Posts with titles found:")
    post_count = 0
    for link in thread_links[:15]:  # Show first 15
        if link['title'] and len(link['title'].strip()) > 0:
            post_count += 1
            print(f"   {post_count:2d}. {link['title'][:50]:<50} -> {link['href']}")
    
    print(f"\n   ðŸ“ˆ Total posts with titles: {post_count}")
    print(f"   ðŸ“ˆ Total thread links: {len(thread_links)}")
    
    # Step 7: Generate final recommendations
    print("\nâœ¨ Step 6: Claude Code Final Recommendations")
    print("-" * 40)
    
    recommendations = {
        'primary_selector': "a[href*='thread-'][href*='-1-1.html']",
        'fallback_selector': "a[href^='thread-']",
        'extraction_strategy': 'Use primary selector for high precision, fallback for completeness',
        'xpath_alternative': "//a[contains(@href, 'thread-') and contains(@href, '-1-1.html')]",
        'estimated_recall': f"{post_count}/{len(thread_links)} = {post_count/max(len(thread_links),1):.1%}",
        'automation_ready': True
    }
    
    print("   ðŸŽ¯ Recommended approach:")
    for key, value in recommendations.items():
        print(f"      {key.replace('_', ' ').title()}: {value}")
    
    # Step 8: Demonstrate practical usage
    print("\nðŸ”§ Step 7: Ready for Playwright Automation")
    print("-" * 40)
    
    playwright_code = f"""
# Generated by Claude Code for reliable post extraction:

# 1. Navigate to forum page
browser_navigate("https://www.1point3acres.com/bbs/tag/openai-9407-1.html")

# 2. Extract all post links using recommended selector
elements = browser_evaluate('''
() => {{
    const links = document.querySelectorAll("{recommendations['primary_selector']}");
    return Array.from(links).map((link, i) => ({{
        index: i,
        title: link.textContent.trim(),
        url: link.href,
        element: link
    }}));
}}
''')

# 3. Click specific posts or iterate through all
# Example: Click first post
browser_click(
    element="Forum post link", 
    ref="a[href*='thread-'][href*='-1-1.html']:first-of-type"
)
"""
    
    print(f"   Generated Playwright automation code:")
    print(f"   ```python{playwright_code}   ```")
    
    return {
        'success': True,
        'posts_found': len(thread_links),
        'posts_with_titles': post_count,
        'recommended_selector': recommendations['primary_selector'],
        'automation_ready': recommendations['automation_ready']
    }


def test_pattern_completeness():
    """
    Test that our pattern recognition doesn't miss any posts
    """
    print("\nðŸ§ª Pattern Completeness Test")
    print("=" * 40)
    
    # Load the test page
    with open('test/page.html', 'r', encoding='gbk') as f:
        html_content = f.read()
    
    # Test different approaches
    approaches = {
        'Manual regex search': len(re.findall(r'thread-\d+-\d+-\d+\.html', html_content)),
        'Our sanitizer approach': len([
            elem for elem in WebExtractionTool().sanitizer.sanitize(html_content, 'links')['element_registry']
            if elem['tag'] == 'a' and 'thread-' in elem['attributes'].get('href', '')
        ]),
        'Simple href pattern': len(re.findall(r'href="thread-[^"]*\.html"', html_content))
    }
    
    print("Comparison of detection methods:")
    for method, count in approaches.items():
        print(f"   {method:<25}: {count:3d} posts")
    
    max_found = max(approaches.values())
    min_found = min(approaches.values())
    
    if max_found == min_found:
        print(f"\n   âœ… All methods agree: {max_found} posts found")
        print(f"   ðŸŽ¯ Our approach has 100% recall")
    else:
        print(f"\n   âš ï¸  Methods disagree: {min_found}-{max_found} posts")
        print(f"   ðŸ” Recommend manual verification")
    
    return max_found


if __name__ == "__main__":
    # Run the complete workflow simulation
    result = claude_code_workflow_simulation()
    
    # Test pattern completeness
    max_posts = test_pattern_completeness()
    
    # Final summary
    print("\nðŸŽ‰ Summary")
    print("=" * 20)
    print(f"âœ… Successfully implemented HTML sanitization for Claude Code")
    print(f"âœ… Detected {result['posts_found']} forum thread links")
    print(f"âœ… Extracted {result['posts_with_titles']} posts with titles")
    print(f"âœ… Generated reliable selector: {result['recommended_selector']}")
    print(f"âœ… Ready for Playwright MCP automation: {result['automation_ready']}")
    print(f"âœ… Estimated recall rate: {result['posts_with_titles']/max_posts:.1%}")
    
    print(f"\nðŸš€ The tool is ready for Claude Code integration!")
    print(f"   â€¢ HTML sanitization reduces context from ~28k to ~4k tokens")
    print(f"   â€¢ Pattern recognition identifies post links reliably") 
    print(f"   â€¢ Playwright MCP integration enables automation")
    print(f"   â€¢ No direct AI content extraction - uses pattern recognition instead")