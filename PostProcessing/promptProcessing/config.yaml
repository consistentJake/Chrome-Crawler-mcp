# Interview Extractor Configuration
# Copy this file to config.local.yaml and fill in your settings

# API Settings
api:
  # Anthropic API key (required)
  api_key: ""  # Set via environment variable or config.local.yaml

  # Base URL (optional, for proxies or alternative endpoints)
  # Default: https://api.anthropic.com (official Anthropic API)
  # Examples:
  #   - OpenRouter: https://openrouter.ai/api/v1
  #   - Custom proxy: https://your-proxy.com/v1
  #   - Local server: http://localhost:8080
  base_url: https://api.moonshot.cn/anthropic  # Set to your proxy URL, or leave as null for official API

  # Model to use
  model: "kimi-k2-0905-preview"
  # Options: claude-sonnet-4-20250514, claude-opus-4-20250514, claude-3-5-haiku-20241022
  # If using OpenRouter or other providers, use their model names

  # Max output tokens per call
  max_tokens: 4096

  # Temperature (lower = more consistent)
  temperature: 0.1

# Processing Settings
processing:
  # Number of posts per LLM call (2-3 recommended)
  posts_per_group: 3

  # Minimum content length to process a post
  min_content_length: 50

  # Delay between API calls (seconds)
  delay_between_calls: 1.0

# Output Settings
output:
  # Output directory
  output_dir: "output"

  # Save intermediate files (markdown inputs, raw responses)
  save_intermediate: true

# Filter Settings (posts/replies to skip)
filters:
  # Skip posts containing these keywords in title/content
  skip_keywords:
    - "新人如何使用"
    - "发错了"
    - "Welcome on board"
    - "如何免费获得"
    - "积分限制"

  # Low-value reply patterns to filter
  low_value_patterns:
    - "感谢楼主"
    - "已加米"
    - "求加米"
    - "顶"
    - "mark"
    - "已私信"
    - "已dm"
